{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTweet-Large"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malexlu314\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModel, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "seed = 12345678\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlzheimersTweetsDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, target_transform=None):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\", normalization=True, use_fast=False)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        self.df = pd.read_csv(root)\n",
    "        self.length = len(self.df)\n",
    "\n",
    "        self.tweets = self.tokenizer(self.df[\"tweet\"].values.tolist(), padding=True, truncation=True, return_tensors='pt')[\"input_ids\"]\n",
    "        self.labels = torch.LongTensor(self.df[\"label\"].values)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            self.tweets = self.transform(self.tweets)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            self.labels = self.target_transform(self.labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tweets[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(test=False):\n",
    "    if test:\n",
    "        return AlzheimersTweetsDataset(\"data/test.csv\")\n",
    "    \n",
    "    trainset = AlzheimersTweetsDataset(\"data/train.csv\")\n",
    "    valset = AlzheimersTweetsDataset(\"data/val.csv\")\n",
    "    return trainset, valset\n",
    "\n",
    "def make_loader(dataset, batch_size):\n",
    "    loader = DataLoader(dataset=dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True, \n",
    "                        pin_memory=True, num_workers=2)\n",
    "    return loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-large\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, config):    \n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    # wandb.watch(model, log=\"all\", log_freq=10)\n",
    "\n",
    "    # Run training and track with wandb\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        model.train()\n",
    "        train_correct, val_correct = 0, 0\n",
    "        for batch, labels in train_loader:\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "\n",
    "            output = model(batch, labels=labels)\n",
    "            loss = output.loss\n",
    "\n",
    "            partial_loss = loss / config.accum\n",
    "            partial_loss.backward()\n",
    "            \n",
    "            predicted = output.logits.argmax(dim=-1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            example_ct += len(batch)\n",
    "            batch_ct += 1\n",
    "\n",
    "            if (batch_ct % config.accum == 0) or (batch_ct == len(train_loader)):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if (batch_ct % (config.log_interval * config.accum)) == 0:\n",
    "                wandb.log({\"epoch\": epoch, \"loss\": loss.item()}, step=example_ct)\n",
    "                print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch, labels in val_loader:\n",
    "                batch, labels = batch.to(device), labels.to(device)\n",
    "                output = model(batch, labels=labels)\n",
    "\n",
    "                predicted = output.logits.argmax(dim=-1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_accuracy = train_correct / len(train_loader.dataset)\n",
    "        val_accuracy = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        wandb.log({\"train_accuracy\": train_accuracy, \"val_accuracy\": val_accuracy}, step=example_ct)\n",
    "        print(f\"Epoch {str(epoch).zfill(2)} Summary: (Train %: {train_accuracy:%}, Val%: {val_accuracy:%})\")\n",
    "\n",
    "        model.save_pretrained(os.path.join(wandb.run.dir, f\"model_{epoch}\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    # Make the data\n",
    "    train, val = get_data()\n",
    "    train_loader = make_loader(train, batch_size=config.batch_size//config.accum)\n",
    "    val_loader = make_loader(val, batch_size=config.batch_size//config.accum)\n",
    "\n",
    "    # Make the model\n",
    "    model = get_model().to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "    \n",
    "    return model, train_loader, val_loader, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "    assert hyperparameters[\"batch_size\"] % hyperparameters[\"accum\"] == 0\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"Alzheimers\", config=hyperparameters):\n",
    "      # access all HPs through wandb.config, so logging matches execution!\n",
    "      config = wandb.config\n",
    "\n",
    "      # make the model, data, and optimization problem\n",
    "      model, train_loader, val_loader, optimizer = make(config)\n",
    "      print(model)\n",
    "\n",
    "      # and use them to train the model\n",
    "      train(model, train_loader, val_loader, optimizer, config)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 32,\n",
    "    \"accum\": 4,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"log_interval\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/alexlu/Documents/Programming/alexlu07/Scires/Alzheimers/wandb/run-20230630_233501-vmyx56qw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alexlu314/Alzheimers/runs/vmyx56qw' target=\"_blank\">different-totem-35</a></strong> to <a href='https://wandb.ai/alexlu314/Alzheimers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alexlu314/Alzheimers' target=\"_blank\">https://wandb.ai/alexlu314/Alzheimers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alexlu314/Alzheimers/runs/vmyx56qw' target=\"_blank\">https://wandb.ai/alexlu314/Alzheimers/runs/vmyx56qw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at vinai/bertweet-large were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification(\n",
      "  (roberta): RobertaModel(\n",
      "    (embeddings): RobertaEmbeddings(\n",
      "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
      "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
      "      (token_type_embeddings): Embedding(1, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): RobertaEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-23): 24 x RobertaLayer(\n",
      "          (attention): RobertaAttention(\n",
      "            (self): RobertaSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): RobertaSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): RobertaIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): RobertaOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier): RobertaClassificationHead(\n",
      "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c6c4f85c854d86aa49ef4056a0201d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 00128 examples: 0.519\n",
      "Loss after 00256 examples: 0.600\n",
      "Loss after 00384 examples: 0.504\n",
      "Loss after 00512 examples: 0.623\n",
      "Loss after 00640 examples: 0.659\n",
      "Loss after 00768 examples: 0.468\n",
      "Loss after 00896 examples: 0.666\n",
      "Loss after 01024 examples: 0.563\n",
      "Loss after 01152 examples: 0.536\n",
      "Loss after 01280 examples: 0.724\n",
      "Loss after 01408 examples: 0.537\n",
      "Loss after 01536 examples: 0.464\n",
      "Loss after 01664 examples: 0.477\n",
      "Loss after 01792 examples: 0.231\n",
      "Loss after 01920 examples: 0.511\n",
      "Loss after 02048 examples: 0.244\n",
      "Loss after 02176 examples: 0.511\n",
      "Loss after 02304 examples: 0.978\n",
      "Loss after 02432 examples: 0.381\n",
      "Loss after 02560 examples: 0.416\n",
      "Loss after 02688 examples: 0.551\n",
      "Loss after 02816 examples: 0.315\n",
      "Loss after 02944 examples: 0.351\n",
      "Loss after 03072 examples: 0.246\n",
      "Loss after 03200 examples: 0.792\n",
      "Loss after 03328 examples: 0.245\n",
      "Loss after 03456 examples: 0.537\n",
      "Loss after 03584 examples: 0.208\n",
      "Loss after 03712 examples: 0.090\n",
      "Loss after 03840 examples: 0.071\n",
      "Loss after 03968 examples: 0.077\n",
      "Loss after 04096 examples: 0.172\n",
      "Loss after 04224 examples: 0.435\n",
      "Epoch 00 Summary: (Train %: 77.911275%, Val%: 90.573013%)\n",
      "Loss after 04352 examples: 0.263\n",
      "Loss after 04480 examples: 0.129\n",
      "Loss after 04608 examples: 0.061\n",
      "Loss after 04736 examples: 0.442\n",
      "Loss after 04864 examples: 0.955\n",
      "Loss after 04992 examples: 0.991\n",
      "Loss after 05120 examples: 0.021\n",
      "Loss after 05248 examples: 0.075\n",
      "Loss after 05376 examples: 0.126\n",
      "Loss after 05504 examples: 0.370\n",
      "Loss after 05632 examples: 0.163\n",
      "Loss after 05760 examples: 0.058\n",
      "Loss after 05888 examples: 0.186\n",
      "Loss after 06016 examples: 0.092\n",
      "Loss after 06144 examples: 0.061\n",
      "Loss after 06272 examples: 0.415\n",
      "Loss after 06400 examples: 0.054\n",
      "Loss after 06528 examples: 0.583\n",
      "Loss after 06656 examples: 0.048\n",
      "Loss after 06784 examples: 0.096\n",
      "Loss after 06912 examples: 0.355\n",
      "Loss after 07040 examples: 0.567\n",
      "Loss after 07168 examples: 1.027\n",
      "Loss after 07296 examples: 0.043\n",
      "Loss after 07424 examples: 0.222\n",
      "Loss after 07552 examples: 0.170\n",
      "Loss after 07680 examples: 0.026\n",
      "Loss after 07808 examples: 0.036\n",
      "Loss after 07936 examples: 0.357\n",
      "Loss after 08064 examples: 0.449\n",
      "Loss after 08192 examples: 0.046\n",
      "Loss after 08320 examples: 0.707\n",
      "Loss after 08448 examples: 0.040\n",
      "Loss after 08576 examples: 0.497\n",
      "Epoch 01 Summary: (Train %: 92.444547%, Val%: 94.085028%)\n",
      "Loss after 08704 examples: 0.154\n",
      "Loss after 08832 examples: 0.063\n",
      "Loss after 08960 examples: 0.018\n",
      "Loss after 09088 examples: 0.103\n",
      "Loss after 09216 examples: 0.016\n",
      "Loss after 09344 examples: 0.013\n",
      "Loss after 09472 examples: 1.165\n",
      "Loss after 09600 examples: 0.015\n",
      "Loss after 09728 examples: 0.080\n",
      "Loss after 09856 examples: 0.047\n",
      "Loss after 09984 examples: 0.051\n",
      "Loss after 10112 examples: 0.103\n",
      "Loss after 10240 examples: 0.014\n",
      "Loss after 10368 examples: 0.010\n",
      "Loss after 10496 examples: 0.062\n",
      "Loss after 10624 examples: 0.054\n",
      "Loss after 10752 examples: 0.294\n",
      "Loss after 10880 examples: 0.096\n",
      "Loss after 11008 examples: 0.091\n",
      "Loss after 11136 examples: 0.049\n",
      "Loss after 11264 examples: 0.200\n",
      "Loss after 11392 examples: 0.031\n",
      "Loss after 11520 examples: 0.027\n",
      "Loss after 11648 examples: 0.369\n",
      "Loss after 11776 examples: 0.079\n",
      "Loss after 11904 examples: 0.075\n",
      "Loss after 12032 examples: 0.047\n",
      "Loss after 12160 examples: 0.011\n",
      "Loss after 12288 examples: 0.033\n",
      "Loss after 12416 examples: 0.155\n",
      "Loss after 12544 examples: 0.204\n",
      "Loss after 12672 examples: 0.032\n",
      "Loss after 12800 examples: 0.010\n",
      "Loss after 12928 examples: 0.005\n",
      "Epoch 02 Summary: (Train %: 95.355823%, Val%: 95.194085%)\n",
      "Loss after 13056 examples: 0.006\n",
      "Loss after 13184 examples: 0.027\n",
      "Loss after 13312 examples: 0.013\n",
      "Loss after 13440 examples: 0.035\n",
      "Loss after 13568 examples: 0.004\n",
      "Loss after 13696 examples: 0.018\n",
      "Loss after 13824 examples: 0.005\n",
      "Loss after 13952 examples: 0.010\n",
      "Loss after 14080 examples: 0.008\n",
      "Loss after 14208 examples: 0.018\n",
      "Loss after 14336 examples: 0.014\n",
      "Loss after 14464 examples: 0.004\n",
      "Loss after 14592 examples: 0.004\n",
      "Loss after 14720 examples: 0.008\n",
      "Loss after 14848 examples: 0.074\n",
      "Loss after 14976 examples: 0.011\n",
      "Loss after 15104 examples: 0.771\n",
      "Loss after 15232 examples: 0.008\n",
      "Loss after 15360 examples: 0.135\n",
      "Loss after 15488 examples: 0.010\n",
      "Loss after 15616 examples: 0.239\n",
      "Loss after 15744 examples: 0.104\n",
      "Loss after 15872 examples: 0.008\n",
      "Loss after 16000 examples: 0.027\n",
      "Loss after 16128 examples: 0.005\n",
      "Loss after 16256 examples: 0.010\n",
      "Loss after 16384 examples: 0.004\n",
      "Loss after 16512 examples: 0.044\n",
      "Loss after 16640 examples: 0.426\n",
      "Loss after 16768 examples: 0.414\n",
      "Loss after 16896 examples: 0.004\n",
      "Loss after 17024 examples: 0.008\n",
      "Loss after 17152 examples: 0.154\n",
      "Loss after 17280 examples: 0.008\n",
      "Epoch 03 Summary: (Train %: 97.458410%, Val%: 93.900185%)\n",
      "Loss after 17408 examples: 0.005\n",
      "Loss after 17536 examples: 0.006\n",
      "Loss after 17664 examples: 0.176\n",
      "Loss after 17792 examples: 0.017\n",
      "Loss after 17920 examples: 0.011\n",
      "Loss after 18048 examples: 0.099\n",
      "Loss after 18176 examples: 0.001\n",
      "Loss after 18304 examples: 0.004\n",
      "Loss after 18432 examples: 0.003\n",
      "Loss after 18560 examples: 0.004\n",
      "Loss after 18688 examples: 0.000\n",
      "Loss after 18816 examples: 0.266\n",
      "Loss after 18944 examples: 0.016\n",
      "Loss after 19072 examples: 0.008\n",
      "Loss after 19200 examples: 0.023\n",
      "Loss after 19328 examples: 0.003\n",
      "Loss after 19456 examples: 0.001\n",
      "Loss after 19584 examples: 0.002\n",
      "Loss after 19712 examples: 0.002\n",
      "Loss after 19840 examples: 0.003\n",
      "Loss after 19968 examples: 0.002\n",
      "Loss after 20096 examples: 0.001\n",
      "Loss after 20224 examples: 0.053\n",
      "Loss after 20352 examples: 0.062\n",
      "Loss after 20480 examples: 0.013\n",
      "Loss after 20608 examples: 0.002\n",
      "Loss after 20736 examples: 0.024\n",
      "Loss after 20864 examples: 0.034\n",
      "Loss after 20992 examples: 0.020\n",
      "Loss after 21120 examples: 0.006\n",
      "Loss after 21248 examples: 0.200\n",
      "Loss after 21376 examples: 0.007\n",
      "Loss after 21504 examples: 0.010\n",
      "Loss after 21632 examples: 0.025\n",
      "Epoch 04 Summary: (Train %: 98.659889%, Val%: 94.639556%)\n",
      "Loss after 21760 examples: 0.006\n",
      "Loss after 21888 examples: 0.001\n",
      "Loss after 22016 examples: 0.002\n",
      "Loss after 22144 examples: 0.012\n",
      "Loss after 22272 examples: 0.175\n",
      "Loss after 22400 examples: 0.001\n",
      "Loss after 22528 examples: 0.004\n",
      "Loss after 22656 examples: 0.004\n",
      "Loss after 22784 examples: 0.001\n",
      "Loss after 22912 examples: 0.002\n",
      "Loss after 23040 examples: 0.003\n",
      "Loss after 23168 examples: 0.007\n",
      "Loss after 23296 examples: 0.002\n",
      "Loss after 23424 examples: 0.791\n",
      "Loss after 23552 examples: 0.029\n",
      "Loss after 23680 examples: 0.007\n",
      "Loss after 23808 examples: 0.116\n",
      "Loss after 23936 examples: 0.010\n",
      "Loss after 24064 examples: 0.005\n",
      "Loss after 24192 examples: 0.027\n",
      "Loss after 24320 examples: 0.004\n",
      "Loss after 24448 examples: 0.067\n",
      "Loss after 24576 examples: 0.001\n",
      "Loss after 24704 examples: 0.001\n",
      "Loss after 24832 examples: 0.004\n",
      "Loss after 24960 examples: 0.002\n",
      "Loss after 25088 examples: 0.001\n",
      "Loss after 25216 examples: 0.007\n",
      "Loss after 25344 examples: 0.002\n",
      "Loss after 25472 examples: 0.006\n",
      "Loss after 25600 examples: 0.001\n",
      "Loss after 25728 examples: 0.002\n",
      "Loss after 25856 examples: 0.018\n",
      "Epoch 05 Summary: (Train %: 99.191312%, Val%: 95.194085%)\n",
      "Loss after 25984 examples: 0.002\n",
      "Loss after 26112 examples: 0.006\n",
      "Loss after 26240 examples: 0.001\n",
      "Loss after 26368 examples: 0.002\n",
      "Loss after 26496 examples: 0.001\n",
      "Loss after 26624 examples: 0.001\n",
      "Loss after 26752 examples: 0.001\n",
      "Loss after 26880 examples: 0.049\n",
      "Loss after 27008 examples: 0.001\n",
      "Loss after 27136 examples: 0.002\n",
      "Loss after 27264 examples: 0.001\n",
      "Loss after 27392 examples: 0.060\n",
      "Loss after 27520 examples: 0.002\n",
      "Loss after 27648 examples: 0.001\n",
      "Loss after 27776 examples: 0.082\n",
      "Loss after 27904 examples: 0.001\n",
      "Loss after 28032 examples: 0.002\n",
      "Loss after 28160 examples: 0.003\n",
      "Loss after 28288 examples: 0.014\n",
      "Loss after 28416 examples: 0.006\n",
      "Loss after 28544 examples: 0.004\n",
      "Loss after 28672 examples: 0.019\n",
      "Loss after 28800 examples: 0.010\n",
      "Loss after 28928 examples: 0.001\n",
      "Loss after 29056 examples: 0.010\n",
      "Loss after 29184 examples: 0.001\n",
      "Loss after 29312 examples: 0.018\n",
      "Loss after 29440 examples: 0.001\n",
      "Loss after 29568 examples: 0.059\n",
      "Loss after 29696 examples: 0.002\n",
      "Loss after 29824 examples: 0.067\n",
      "Loss after 29952 examples: 0.001\n",
      "Loss after 30080 examples: 0.005\n",
      "Loss after 30208 examples: 0.000\n",
      "Epoch 06 Summary: (Train %: 99.306839%, Val%: 94.454713%)\n",
      "Loss after 30336 examples: 0.001\n",
      "Loss after 30464 examples: 0.101\n",
      "Loss after 30592 examples: 0.005\n",
      "Loss after 30720 examples: 0.001\n",
      "Loss after 30848 examples: 0.018\n",
      "Loss after 30976 examples: 0.001\n",
      "Loss after 31104 examples: 0.000\n",
      "Loss after 31232 examples: 0.000\n",
      "Loss after 31360 examples: 0.000\n",
      "Loss after 31488 examples: 0.020\n",
      "Loss after 31616 examples: 0.002\n",
      "Loss after 31744 examples: 0.001\n",
      "Loss after 31872 examples: 0.000\n",
      "Loss after 32000 examples: 0.001\n",
      "Loss after 32128 examples: 0.000\n",
      "Loss after 32256 examples: 0.000\n",
      "Loss after 32384 examples: 0.000\n",
      "Loss after 32512 examples: 0.001\n",
      "Loss after 32640 examples: 0.090\n",
      "Loss after 32768 examples: 0.001\n",
      "Loss after 32896 examples: 0.000\n",
      "Loss after 33024 examples: 0.181\n",
      "Loss after 33152 examples: 0.001\n",
      "Loss after 33280 examples: 0.593\n",
      "Loss after 33408 examples: 0.003\n",
      "Loss after 33536 examples: 0.005\n",
      "Loss after 33664 examples: 0.033\n",
      "Loss after 33792 examples: 0.015\n",
      "Loss after 33920 examples: 0.007\n",
      "Loss after 34048 examples: 0.004\n",
      "Loss after 34176 examples: 0.020\n",
      "Loss after 34304 examples: 0.028\n",
      "Loss after 34432 examples: 0.025\n",
      "Loss after 34560 examples: 0.003\n",
      "Epoch 07 Summary: (Train %: 99.145102%, Val%: 94.454713%)\n",
      "Loss after 34688 examples: 0.047\n",
      "Loss after 34816 examples: 0.005\n",
      "Loss after 34944 examples: 0.001\n",
      "Loss after 35072 examples: 0.002\n",
      "Loss after 35200 examples: 0.024\n",
      "Loss after 35328 examples: 0.001\n",
      "Loss after 35456 examples: 0.334\n",
      "Loss after 35584 examples: 0.149\n",
      "Loss after 35712 examples: 0.422\n",
      "Loss after 35840 examples: 0.008\n",
      "Loss after 35968 examples: 0.110\n",
      "Loss after 36096 examples: 0.112\n",
      "Loss after 36224 examples: 0.047\n",
      "Loss after 36352 examples: 0.006\n",
      "Loss after 36480 examples: 0.004\n",
      "Loss after 36608 examples: 0.089\n",
      "Loss after 36736 examples: 0.003\n",
      "Loss after 36864 examples: 0.553\n",
      "Loss after 36992 examples: 0.005\n",
      "Loss after 37120 examples: 0.004\n",
      "Loss after 37248 examples: 0.014\n",
      "Loss after 37376 examples: 0.089\n",
      "Loss after 37504 examples: 0.001\n",
      "Loss after 37632 examples: 0.019\n",
      "Loss after 37760 examples: 0.001\n",
      "Loss after 37888 examples: 0.005\n",
      "Loss after 38016 examples: 0.019\n",
      "Loss after 38144 examples: 0.006\n",
      "Loss after 38272 examples: 0.100\n",
      "Loss after 38400 examples: 0.025\n",
      "Loss after 38528 examples: 0.034\n",
      "Loss after 38656 examples: 0.000\n",
      "Loss after 38784 examples: 0.004\n",
      "Loss after 38912 examples: 0.005\n",
      "Epoch 08 Summary: (Train %: 98.012939%, Val%: 94.269871%)\n",
      "Loss after 39040 examples: 0.000\n",
      "Loss after 39168 examples: 0.002\n",
      "Loss after 39296 examples: 0.162\n",
      "Loss after 39424 examples: 0.000\n",
      "Loss after 39552 examples: 0.000\n",
      "Loss after 39680 examples: 0.001\n",
      "Loss after 39808 examples: 0.001\n",
      "Loss after 39936 examples: 0.000\n",
      "Loss after 40064 examples: 0.008\n",
      "Loss after 40192 examples: 0.000\n",
      "Loss after 40320 examples: 0.001\n",
      "Loss after 40448 examples: 0.001\n",
      "Loss after 40576 examples: 0.363\n",
      "Loss after 40704 examples: 0.004\n",
      "Loss after 40832 examples: 0.007\n",
      "Loss after 40960 examples: 0.420\n",
      "Loss after 41088 examples: 0.004\n",
      "Loss after 41216 examples: 0.007\n",
      "Loss after 41344 examples: 0.013\n",
      "Loss after 41472 examples: 0.001\n",
      "Loss after 41600 examples: 0.031\n",
      "Loss after 41728 examples: 0.274\n",
      "Loss after 41856 examples: 0.001\n",
      "Loss after 41984 examples: 0.001\n",
      "Loss after 42112 examples: 0.020\n",
      "Loss after 42240 examples: 0.016\n",
      "Loss after 42368 examples: 0.001\n",
      "Loss after 42496 examples: 0.000\n",
      "Loss after 42624 examples: 0.000\n",
      "Loss after 42752 examples: 0.346\n",
      "Loss after 42880 examples: 0.000\n",
      "Loss after 43008 examples: 0.083\n",
      "Loss after 43136 examples: 0.000\n",
      "Loss after 43264 examples: 0.001\n",
      "Epoch 09 Summary: (Train %: 99.191312%, Val%: 95.378928%)\n",
      "Loss after 43392 examples: 0.000\n",
      "Loss after 43520 examples: 0.001\n",
      "Loss after 43648 examples: 0.000\n",
      "Loss after 43776 examples: 0.007\n",
      "Loss after 43904 examples: 0.001\n",
      "Loss after 44032 examples: 0.010\n",
      "Loss after 44160 examples: 0.095\n",
      "Loss after 44288 examples: 0.001\n",
      "Loss after 44416 examples: 0.001\n",
      "Loss after 44544 examples: 0.005\n",
      "Loss after 44672 examples: 0.001\n",
      "Loss after 44800 examples: 0.001\n",
      "Loss after 44928 examples: 0.000\n",
      "Loss after 45056 examples: 0.042\n",
      "Loss after 45184 examples: 0.000\n",
      "Loss after 45312 examples: 0.003\n",
      "Loss after 45440 examples: 0.001\n",
      "Loss after 45568 examples: 0.001\n",
      "Loss after 45696 examples: 0.000\n",
      "Loss after 45824 examples: 0.000\n",
      "Loss after 45952 examples: 0.000\n",
      "Loss after 46080 examples: 0.006\n",
      "Loss after 46208 examples: 0.000\n",
      "Loss after 46336 examples: 0.005\n",
      "Loss after 46464 examples: 0.224\n",
      "Loss after 46592 examples: 0.000\n",
      "Loss after 46720 examples: 0.000\n",
      "Loss after 46848 examples: 0.000\n",
      "Loss after 46976 examples: 0.000\n",
      "Loss after 47104 examples: 0.000\n",
      "Loss after 47232 examples: 0.001\n",
      "Loss after 47360 examples: 0.000\n",
      "Loss after 47488 examples: 0.000\n",
      "Epoch 10 Summary: (Train %: 99.537893%, Val%: 95.009242%)\n",
      "Loss after 47616 examples: 0.000\n",
      "Loss after 47744 examples: 0.034\n",
      "Loss after 47872 examples: 0.044\n",
      "Loss after 48000 examples: 0.000\n",
      "Loss after 48128 examples: 0.001\n",
      "Loss after 48256 examples: 0.000\n",
      "Loss after 48384 examples: 0.000\n",
      "Loss after 48512 examples: 0.000\n",
      "Loss after 48640 examples: 0.000\n",
      "Loss after 48768 examples: 0.000\n",
      "Loss after 48896 examples: 0.000\n",
      "Loss after 49024 examples: 0.000\n",
      "Loss after 49152 examples: 0.000\n",
      "Loss after 49280 examples: 0.000\n",
      "Loss after 49408 examples: 0.000\n",
      "Loss after 49536 examples: 0.000\n",
      "Loss after 49664 examples: 0.000\n",
      "Loss after 49792 examples: 0.002\n",
      "Loss after 49920 examples: 0.009\n",
      "Loss after 50048 examples: 0.000\n",
      "Loss after 50176 examples: 0.000\n",
      "Loss after 50304 examples: 0.000\n",
      "Loss after 50432 examples: 0.001\n",
      "Loss after 50560 examples: 0.000\n",
      "Loss after 50688 examples: 0.000\n",
      "Loss after 50816 examples: 0.001\n",
      "Loss after 50944 examples: 0.003\n",
      "Loss after 51072 examples: 0.002\n",
      "Loss after 51200 examples: 0.007\n",
      "Loss after 51328 examples: 0.001\n",
      "Loss after 51456 examples: 0.001\n",
      "Loss after 51584 examples: 0.001\n",
      "Loss after 51712 examples: 0.000\n",
      "Loss after 51840 examples: 0.211\n",
      "Epoch 11 Summary: (Train %: 99.491682%, Val%: 94.824399%)\n",
      "Loss after 51968 examples: 0.013\n",
      "Loss after 52096 examples: 0.007\n",
      "Loss after 52224 examples: 0.001\n",
      "Loss after 52352 examples: 0.003\n",
      "Loss after 52480 examples: 0.001\n",
      "Loss after 52608 examples: 0.000\n",
      "Loss after 52736 examples: 0.000\n",
      "Loss after 52864 examples: 0.000\n",
      "Loss after 52992 examples: 0.000\n",
      "Loss after 53120 examples: 0.000\n",
      "Loss after 53248 examples: 0.000\n",
      "Loss after 53376 examples: 0.000\n",
      "Loss after 53504 examples: 0.000\n",
      "Loss after 53632 examples: 0.013\n",
      "Loss after 53760 examples: 0.001\n",
      "Loss after 53888 examples: 0.000\n",
      "Loss after 54016 examples: 0.000\n",
      "Loss after 54144 examples: 0.001\n",
      "Loss after 54272 examples: 0.002\n",
      "Loss after 54400 examples: 0.004\n",
      "Loss after 54528 examples: 0.001\n",
      "Loss after 54656 examples: 0.000\n",
      "Loss after 54784 examples: 0.001\n",
      "Loss after 54912 examples: 0.000\n",
      "Loss after 55040 examples: 0.000\n",
      "Loss after 55168 examples: 0.001\n",
      "Loss after 55296 examples: 0.002\n",
      "Loss after 55424 examples: 0.000\n",
      "Loss after 55552 examples: 0.000\n",
      "Loss after 55680 examples: 0.000\n",
      "Loss after 55808 examples: 0.003\n",
      "Loss after 55936 examples: 0.002\n",
      "Loss after 56064 examples: 0.072\n",
      "Loss after 56192 examples: 0.001\n",
      "Epoch 12 Summary: (Train %: 99.584104%, Val%: 94.639556%)\n",
      "Loss after 56320 examples: 0.000\n",
      "Loss after 56448 examples: 0.000\n",
      "Loss after 56576 examples: 0.001\n",
      "Loss after 56704 examples: 0.000\n",
      "Loss after 56832 examples: 0.001\n",
      "Loss after 56960 examples: 0.000\n",
      "Loss after 57088 examples: 0.000\n",
      "Loss after 57216 examples: 0.001\n",
      "Loss after 57344 examples: 0.000\n",
      "Loss after 57472 examples: 0.000\n",
      "Loss after 57600 examples: 0.000\n",
      "Loss after 57728 examples: 0.000\n",
      "Loss after 57856 examples: 0.000\n",
      "Loss after 57984 examples: 0.002\n",
      "Loss after 58112 examples: 0.000\n",
      "Loss after 58240 examples: 0.000\n",
      "Loss after 58368 examples: 0.000\n",
      "Loss after 58496 examples: 0.000\n",
      "Loss after 58624 examples: 0.000\n",
      "Loss after 58752 examples: 0.000\n",
      "Loss after 58880 examples: 0.000\n",
      "Loss after 59008 examples: 0.000\n",
      "Loss after 59136 examples: 0.000\n",
      "Loss after 59264 examples: 0.000\n",
      "Loss after 59392 examples: 0.000\n",
      "Loss after 59520 examples: 0.000\n",
      "Loss after 59648 examples: 0.001\n",
      "Loss after 59776 examples: 0.002\n",
      "Loss after 59904 examples: 0.000\n",
      "Loss after 60032 examples: 0.000\n",
      "Loss after 60160 examples: 0.000\n",
      "Loss after 60288 examples: 0.000\n",
      "Loss after 60416 examples: 0.001\n",
      "Loss after 60544 examples: 0.000\n",
      "Epoch 13 Summary: (Train %: 99.930684%, Val%: 95.194085%)\n",
      "Loss after 60672 examples: 0.000\n",
      "Loss after 60800 examples: 0.001\n",
      "Loss after 60928 examples: 0.000\n",
      "Loss after 61056 examples: 0.000\n",
      "Loss after 61184 examples: 0.000\n",
      "Loss after 61312 examples: 0.000\n",
      "Loss after 61440 examples: 0.000\n",
      "Loss after 61568 examples: 0.000\n",
      "Loss after 61696 examples: 0.006\n",
      "Loss after 61824 examples: 0.000\n",
      "Loss after 61952 examples: 0.000\n",
      "Loss after 62080 examples: 0.000\n",
      "Loss after 62208 examples: 0.000\n",
      "Loss after 62336 examples: 0.000\n",
      "Loss after 62464 examples: 0.000\n",
      "Loss after 62592 examples: 0.000\n",
      "Loss after 62720 examples: 0.000\n",
      "Loss after 62848 examples: 0.000\n",
      "Loss after 62976 examples: 0.000\n",
      "Loss after 63104 examples: 0.000\n",
      "Loss after 63232 examples: 0.000\n",
      "Loss after 63360 examples: 0.000\n",
      "Loss after 63488 examples: 0.000\n",
      "Loss after 63616 examples: 0.000\n",
      "Loss after 63744 examples: 0.000\n",
      "Loss after 63872 examples: 0.000\n",
      "Loss after 64000 examples: 0.000\n",
      "Loss after 64128 examples: 0.000\n",
      "Loss after 64256 examples: 0.000\n",
      "Loss after 64384 examples: 0.001\n",
      "Loss after 64512 examples: 0.000\n",
      "Loss after 64640 examples: 0.003\n",
      "Loss after 64768 examples: 0.000\n",
      "Loss after 64896 examples: 0.000\n",
      "Epoch 14 Summary: (Train %: 99.930684%, Val%: 95.194085%)\n",
      "Loss after 65024 examples: 0.000\n",
      "Loss after 65152 examples: 0.000\n",
      "Loss after 65280 examples: 0.001\n",
      "Loss after 65408 examples: 0.000\n",
      "Loss after 65536 examples: 0.000\n",
      "Loss after 65664 examples: 0.000\n",
      "Loss after 65792 examples: 0.000\n",
      "Loss after 65920 examples: 0.000\n",
      "Loss after 66048 examples: 0.000\n",
      "Loss after 66176 examples: 0.000\n",
      "Loss after 66304 examples: 0.000\n",
      "Loss after 66432 examples: 0.000\n",
      "Loss after 66560 examples: 0.000\n",
      "Loss after 66688 examples: 0.000\n",
      "Loss after 66816 examples: 0.006\n",
      "Loss after 66944 examples: 0.000\n",
      "Loss after 67072 examples: 0.000\n",
      "Loss after 67200 examples: 0.000\n",
      "Loss after 67328 examples: 0.000\n",
      "Loss after 67456 examples: 0.000\n",
      "Loss after 67584 examples: 0.000\n",
      "Loss after 67712 examples: 0.000\n",
      "Loss after 67840 examples: 0.001\n",
      "Loss after 67968 examples: 0.005\n",
      "Loss after 68096 examples: 0.000\n",
      "Loss after 68224 examples: 0.000\n",
      "Loss after 68352 examples: 0.000\n",
      "Loss after 68480 examples: 0.000\n",
      "Loss after 68608 examples: 0.000\n",
      "Loss after 68736 examples: 0.000\n",
      "Loss after 68864 examples: 0.000\n",
      "Loss after 68992 examples: 0.000\n",
      "Loss after 69120 examples: 0.000\n",
      "Loss after 69248 examples: 0.000\n",
      "Epoch 15 Summary: (Train %: 99.953789%, Val%: 95.009242%)\n",
      "Loss after 69376 examples: 0.000\n",
      "Loss after 69504 examples: 0.000\n",
      "Loss after 69632 examples: 0.000\n",
      "Loss after 69760 examples: 0.000\n",
      "Loss after 69888 examples: 0.000\n",
      "Loss after 70016 examples: 0.001\n",
      "Loss after 70144 examples: 0.000\n",
      "Loss after 70272 examples: 0.000\n",
      "Loss after 70400 examples: 0.001\n",
      "Loss after 70528 examples: 0.000\n",
      "Loss after 70656 examples: 0.000\n",
      "Loss after 70784 examples: 0.000\n",
      "Loss after 70912 examples: 0.000\n",
      "Loss after 71040 examples: 0.001\n",
      "Loss after 71168 examples: 0.000\n",
      "Loss after 71296 examples: 0.000\n",
      "Loss after 71424 examples: 0.000\n",
      "Loss after 71552 examples: 0.000\n",
      "Loss after 71680 examples: 0.000\n",
      "Loss after 71808 examples: 0.000\n",
      "Loss after 71936 examples: 0.000\n",
      "Loss after 72064 examples: 0.000\n",
      "Loss after 72192 examples: 0.001\n",
      "Loss after 72320 examples: 0.000\n",
      "Loss after 72448 examples: 0.000\n",
      "Loss after 72576 examples: 0.000\n",
      "Loss after 72704 examples: 0.000\n",
      "Loss after 72832 examples: 0.001\n",
      "Loss after 72960 examples: 0.000\n",
      "Loss after 73088 examples: 0.000\n",
      "Loss after 73216 examples: 0.000\n",
      "Loss after 73344 examples: 0.000\n",
      "Loss after 73472 examples: 0.000\n",
      "Epoch 16 Summary: (Train %: 99.953789%, Val%: 94.824399%)\n",
      "Loss after 73600 examples: 0.000\n",
      "Loss after 73728 examples: 0.000\n",
      "Loss after 73856 examples: 0.000\n",
      "Loss after 73984 examples: 0.000\n",
      "Loss after 74112 examples: 0.000\n",
      "Loss after 74240 examples: 0.000\n",
      "Loss after 74368 examples: 0.003\n",
      "Loss after 74496 examples: 0.000\n",
      "Loss after 74624 examples: 0.137\n",
      "Loss after 74752 examples: 0.000\n",
      "Loss after 74880 examples: 0.000\n",
      "Loss after 75008 examples: 0.000\n",
      "Loss after 75136 examples: 0.000\n",
      "Loss after 75264 examples: 0.000\n",
      "Loss after 75392 examples: 0.002\n",
      "Loss after 75520 examples: 0.000\n",
      "Loss after 75648 examples: 0.002\n",
      "Loss after 75776 examples: 0.000\n",
      "Loss after 75904 examples: 0.000\n",
      "Loss after 76032 examples: 0.000\n",
      "Loss after 76160 examples: 0.000\n",
      "Loss after 76288 examples: 0.003\n",
      "Loss after 76416 examples: 0.000\n",
      "Loss after 76544 examples: 0.000\n",
      "Loss after 76672 examples: 0.014\n",
      "Loss after 76800 examples: 0.001\n",
      "Loss after 76928 examples: 0.000\n",
      "Loss after 77056 examples: 0.000\n",
      "Loss after 77184 examples: 0.000\n",
      "Loss after 77312 examples: 0.001\n",
      "Loss after 77440 examples: 0.030\n",
      "Loss after 77568 examples: 0.000\n",
      "Loss after 77696 examples: 0.000\n",
      "Loss after 77824 examples: 0.001\n",
      "Epoch 17 Summary: (Train %: 99.630314%, Val%: 95.194085%)\n",
      "Loss after 77952 examples: 0.001\n",
      "Loss after 78080 examples: 0.000\n",
      "Loss after 78208 examples: 0.000\n",
      "Loss after 78336 examples: 0.000\n",
      "Loss after 78464 examples: 0.002\n",
      "Loss after 78592 examples: 0.000\n",
      "Loss after 78720 examples: 0.000\n",
      "Loss after 78848 examples: 0.000\n",
      "Loss after 78976 examples: 0.000\n",
      "Loss after 79104 examples: 0.000\n",
      "Loss after 79232 examples: 0.000\n",
      "Loss after 79360 examples: 0.105\n",
      "Loss after 79488 examples: 0.000\n",
      "Loss after 79616 examples: 0.000\n",
      "Loss after 79744 examples: 0.002\n",
      "Loss after 79872 examples: 0.000\n",
      "Loss after 80000 examples: 0.054\n",
      "Loss after 80128 examples: 0.000\n",
      "Loss after 80256 examples: 0.000\n",
      "Loss after 80384 examples: 0.000\n",
      "Loss after 80512 examples: 0.000\n",
      "Loss after 80640 examples: 0.000\n",
      "Loss after 80768 examples: 0.000\n",
      "Loss after 80896 examples: 0.000\n",
      "Loss after 81024 examples: 0.001\n",
      "Loss after 81152 examples: 0.000\n",
      "Loss after 81280 examples: 0.000\n",
      "Loss after 81408 examples: 0.000\n",
      "Loss after 81536 examples: 0.000\n",
      "Loss after 81664 examples: 0.001\n",
      "Loss after 81792 examples: 0.000\n",
      "Loss after 81920 examples: 0.000\n",
      "Loss after 82048 examples: 0.001\n",
      "Loss after 82176 examples: 0.002\n",
      "Epoch 18 Summary: (Train %: 99.768946%, Val%: 95.378928%)\n",
      "Loss after 82304 examples: 0.005\n",
      "Loss after 82432 examples: 0.001\n",
      "Loss after 82560 examples: 0.002\n",
      "Loss after 82688 examples: 0.012\n",
      "Loss after 82816 examples: 0.001\n",
      "Loss after 82944 examples: 0.007\n",
      "Loss after 83072 examples: 0.000\n",
      "Loss after 83200 examples: 0.000\n",
      "Loss after 83328 examples: 0.000\n",
      "Loss after 83456 examples: 0.001\n",
      "Loss after 83584 examples: 0.000\n",
      "Loss after 83712 examples: 0.001\n",
      "Loss after 83840 examples: 0.000\n",
      "Loss after 83968 examples: 0.000\n",
      "Loss after 84096 examples: 0.004\n",
      "Loss after 84224 examples: 0.001\n",
      "Loss after 84352 examples: 0.000\n",
      "Loss after 84480 examples: 0.000\n",
      "Loss after 84608 examples: 0.000\n",
      "Loss after 84736 examples: 0.001\n",
      "Loss after 84864 examples: 0.003\n",
      "Loss after 84992 examples: 0.000\n",
      "Loss after 85120 examples: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Control-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>▅▃▁█▁▂▁▂▁▁▁▁▁▁▁▁▁▅▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▆▇▇████▇██████████</td></tr><tr><td>val_accuracy</td><td>▁▆█▆▇█▇▇▆█▇▇▇██▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>loss</td><td>0.00056</td></tr><tr><td>train_accuracy</td><td>0.99769</td></tr><tr><td>val_accuracy</td><td>0.95379</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">different-totem-35</strong> at: <a href='https://wandb.ai/alexlu314/Alzheimers/runs/vmyx56qw' target=\"_blank\">https://wandb.ai/alexlu314/Alzheimers/runs/vmyx56qw</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230630_233501-vmyx56qw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/alexlu/Documents/Programming/alexlu07/Scires/Alzheimers/BERTweet.ipynb Cell 17\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/Alzheimers/BERTweet.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m model_pipeline(config)\n",
      "\u001b[1;32m/home/alexlu/Documents/Programming/alexlu07/Scires/Alzheimers/BERTweet.ipynb Cell 17\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/Alzheimers/BERTweet.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   \u001b[39mprint\u001b[39m(model)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/Alzheimers/BERTweet.ipynb#X22sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m   \u001b[39m# and use them to train the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/Alzheimers/BERTweet.ipynb#X22sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m   train(model, train_loader, val_loader, optimizer, config)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/Alzheimers/BERTweet.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mreturn\u001b[39;00m model\n",
      "\u001b[1;32m/home/alexlu/Documents/Programming/alexlu07/Scires/Alzheimers/BERTweet.ipynb Cell 17\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/Alzheimers/BERTweet.ipynb#X22sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mloss\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/Alzheimers/BERTweet.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m partial_loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m config\u001b[39m.\u001b[39maccum\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/Alzheimers/BERTweet.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m partial_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/Alzheimers/BERTweet.ipynb#X22sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m predicted \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/alexlu/Documents/Programming/alexlu07/Scires/Alzheimers/BERTweet.ipynb#X22sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m train_correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (predicted \u001b[39m==\u001b[39m labels)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/alzheimers/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/alzheimers/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "model = model_pipeline(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(config, model):\n",
    "    test = get_data(test=True)\n",
    "    test_loader = make_loader(test, batch_size=config.batch_size//config.accum)\n",
    "\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, labels in test_loader:\n",
    "            batch, labels = batch.to(device), labels.to(device)\n",
    "            output = model(batch, labels=labels)\n",
    "\n",
    "            predicted = output.logits.argmax(dim=-1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    wandb.log({\"test_accuracy\": accuracy})\n",
    "    print(f\"Final Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"results/model_9/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
