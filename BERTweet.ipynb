{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTweet-Large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from captum.attr import *\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, logging\n",
    "from pydictobject import DictObject\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from BERTweet.TweetNormalizer import *\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "seed = 12345678\n",
    "# seed = 87654321\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_memory():\n",
    "    print(torch.cuda.memory_allocated() / 1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlzheimersTweetsDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, target_transform=None, padding=True, max_length=200):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\", use_fast=False)\n",
    "        self.tokenizer.model_max_length = 512\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        if root[-4:] == \".csv\":\n",
    "            self.df = pd.read_csv(root)\n",
    "        elif root[-5:] == \".xlsx\":\n",
    "            self.df = pd.read_excel(root)\n",
    "        else:\n",
    "            self.df = None\n",
    "\n",
    "        self.length = len(self.df)\n",
    "\n",
    "        self.tokens = self.tokenizer(normalizeTweet(self.df[\"tweet\"].values), padding=padding, max_length=max_length, truncation=True, return_tensors='pt')\n",
    "        self.tweets = self.tokens['input_ids']\n",
    "        self.amasks = self.tokens['attention_mask']\n",
    "        self.labels = torch.LongTensor(self.df[\"label\"].values)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            self.tweets = self.transform(self.tweets)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            self.labels = self.target_transform(self.labels)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tweets[idx], self.amasks[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path=None, augment=None, **kwargs):\n",
    "    if path:\n",
    "        if path == \"test\":\n",
    "            return AlzheimersTweetsDataset(\"data/test.csv\", **kwargs)\n",
    "        \n",
    "        return AlzheimersTweetsDataset(path, **kwargs)\n",
    "    \n",
    "    if augment == 50/50:\n",
    "        trainset = AlzheimersTweetsDataset(\"data/train_augment_5050.csv\", **kwargs)\n",
    "    elif augment == 75/25:\n",
    "        trainset = AlzheimersTweetsDataset(\"data/train_augment_7525.csv\", **kwargs)\n",
    "    elif augment == \"ta\":\n",
    "        trainset = AlzheimersTweetsDataset(\"data/train_ta.csv\", **kwargs)\n",
    "    elif augment == \"sentiment140\":\n",
    "        trainset = AlzheimersTweetsDataset(\"data/train_sentiment140.csv\", **kwargs)\n",
    "    elif augment == \"parental\":\n",
    "        trainset = AlzheimersTweetsDataset(\"data/train_parental.csv\", **kwargs)\n",
    "    else:\n",
    "        trainset = AlzheimersTweetsDataset(\"data/train.csv\", **kwargs)\n",
    "        \n",
    "    valset = AlzheimersTweetsDataset(\"data/val.csv\", **kwargs)\n",
    "    return trainset, valset\n",
    "\n",
    "def make_loader(dataset, batch_size):\n",
    "    loader = DataLoader(dataset=dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True, \n",
    "                        pin_memory=True, num_workers=2)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, scheduler, config):    \n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    # wandb.watch(model, log=\"all\", log_freq=10)\n",
    "\n",
    "    best_epoch = None\n",
    "    best_val_accuracy = -1\n",
    "\n",
    "    # Run training and track with wandb\n",
    "    example_ct = 0  # number of examples seen\n",
    "    batch_ct = 0\n",
    "\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        model.train()\n",
    "        train_correct, val_correct = 0, 0\n",
    "        for batch, masks, labels in train_loader:\n",
    "            batch, masks, labels = batch.to(device), masks.to(device), labels.to(device)\n",
    "\n",
    "            output = model(batch, attention_mask=masks, labels=labels)\n",
    "\n",
    "            loss = output.loss\n",
    "\n",
    "            partial_loss = loss / config.accum\n",
    "            partial_loss.backward()\n",
    "            \n",
    "            predicted = output.logits.argmax(dim=-1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            example_ct += len(batch)\n",
    "            batch_ct += 1\n",
    "\n",
    "            if (batch_ct % config.accum == 0) or (batch_ct == len(train_loader)):\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if (batch_ct % (config.log_interval * config.accum)) == 0:\n",
    "                wandb.log({\"epoch\": epoch, \"loss\": loss.item()}, step=example_ct)\n",
    "                print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for batch, masks, labels in val_loader:\n",
    "                batch, masks, labels = batch.to(device), masks.to(device), labels.to(device)\n",
    "                output = model(batch, attention_mask=masks, labels=labels)\n",
    "\n",
    "                predicted = output.logits.argmax(dim=-1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_accuracy = train_correct / len(train_loader.dataset)\n",
    "        val_accuracy = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_epoch = epoch\n",
    "            best_val_accuracy = val_accuracy\n",
    "\n",
    "        wandb.log({\"train_accuracy\": train_accuracy, \"val_accuracy\": val_accuracy}, step=example_ct)\n",
    "        print(f\"Epoch {str(epoch).zfill(2)} Summary: (Train %: {train_accuracy:%}, Val%: {val_accuracy:%})\")\n",
    "\n",
    "        # model.save_pretrained(os.path.join(wandb.run.dir, f\"model_{epoch}\"))\n",
    "        model.save_pretrained(os.path.join(\"results\", f\"model_{epoch}\"))\n",
    "\n",
    "    return best_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(config, model, data_dir=\"test\", use_wandb=True, print_str=True):\n",
    "    if not use_wandb:\n",
    "        config = DictObject(config)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test = get_data(data_dir)\n",
    "    test_loader = make_loader(test, batch_size=config.batch_size//config.accum)\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, masks, labels in test_loader:\n",
    "            batch, masks, labels = batch.to(device), masks.to(device), labels.to(device)\n",
    "            output = model(batch, labels=labels)\n",
    "\n",
    "            predicted = output.logits.argmax(dim=-1)\n",
    "\n",
    "            y_true.extend(labels.cpu().tolist())\n",
    "            y_pred.extend(predicted.cpu().tolist())\n",
    "\n",
    "    accuracy = sum([i == j for i, j in zip(y_true, y_pred)]) / len(y_true)\n",
    "\n",
    "    if use_wandb:\n",
    "        wandb.log({\"test_accuracy\": accuracy})\n",
    "\n",
    "    if print_str:\n",
    "        print(classification_report(y_true, y_pred))\n",
    "    else:\n",
    "        return accuracy, classification_report(y_true, y_pred, output_dict=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make(config):\n",
    "    # Make the data\n",
    "    train, val = get_data(augment=config.augment)\n",
    "    train_loader = make_loader(train, batch_size=config.batch_size//config.accum)\n",
    "    val_loader = make_loader(val, batch_size=config.batch_size//config.accum)\n",
    "\n",
    "    # Make the model\n",
    "    model = get_model().to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "    if config.scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, \n",
    "                                                      start_factor=config.scheduler[0], \n",
    "                                                      end_factor=config.scheduler[1],\n",
    "                                                      total_iters=config.scheduler[2])\n",
    "    else:\n",
    "        scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, 1, 1, 0)\n",
    "    \n",
    "    return model, train_loader, val_loader, optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters):\n",
    "    assert hyperparameters[\"batch_size\"] % hyperparameters[\"accum\"] == 0\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"Alzheimers\", config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # make the model, data, and optimization problem\n",
    "        model, train_loader, val_loader, optimizer, scheduler = make(config)\n",
    "        print(model)\n",
    "\n",
    "        # and use them to train the model\n",
    "        best_epoch = train(model, train_loader, val_loader, optimizer, scheduler, config)\n",
    "        print(\"Best Epoch:\", best_epoch)\n",
    "\n",
    "        # and run test its final performance\n",
    "        # model.from_pretrained(os.path.join(wandb.run.dir, f\"model_{best_epoch}\"))\n",
    "        model = model.from_pretrained(os.path.join(\"results\", f\"model_{best_epoch}\")).to(device)\n",
    "        test(config, model)\n",
    "      \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epochs\": 40,\n",
    "    \"batch_size\": 32,\n",
    "    \"accum\": 1,\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"log_interval\": 4,\n",
    "    \"augment\": \"parental\",\n",
    "    # \"scheduler\": [1, 0.1, 25, \"linear\"]\n",
    "    \"scheduler\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_pipeline(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model().from_pretrained(\"results_sentiment140/model_13/\").to(device)\n",
    "# model = get_model().from_pretrained(\"results_mask(1-10 are corrupted)/model_2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(config, model, use_wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(config, model, data_dir=\"test_generalization2.xlsx\", use_wandb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tes = []\n",
    "# gen = []\n",
    "\n",
    "# for i in tqdm(range(40)):\n",
    "#     model = get_model().from_pretrained(f\"results_sentiment140/model_{i}/\").to(device)\n",
    "#     tes.append(test(config, model, use_wandb=False, print_str=False)[0])\n",
    "#     gen.append(test(config, model, data_dir=\"test_generalization2.xlsx\", use_wandb=False, print_str=False)[0])\n",
    "\n",
    "# df = pd.DataFrame({\"test\": tes, \"gen\": gen})\n",
    "# df.plot.line(subplots=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-large\", use_fast=False)\n",
    "tokenizer.model_max_length = 512\n",
    "\n",
    "PAD_IND = tokenizer.encode(\"<pad>\")[1]\n",
    "token_reference = TokenReferenceBase(reference_token_idx=PAD_IND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(lambda x, attention_mask=None: model(x, attention_mask=attention_mask).logits, model.roberta.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumalate couple samples in this array for visualization purposes\n",
    "\n",
    "def interpret_sentences(model, sentences, masks, labels, tokenize=[]):\n",
    "    log_memory()\n",
    "    classes = [\"negative\", \"positive\"]\n",
    "\n",
    "    for i in tokenize:\n",
    "        tokenized = tokenizer(normalizeTweet(sentences[i]), padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "        sentences[i] = tokenized['input_ids']\n",
    "        masks[i] = tokenized['attention_mask']\n",
    "        \n",
    "    if isinstance(sentences, list):\n",
    "        sentences = torch.stack(sentences)\n",
    "        masks = torch.stack(masks)\n",
    "\n",
    "    sentences = sentences.to(device)\n",
    "    masks = masks.to(device)\n",
    "\n",
    "    text = [[tokenizer.decode(word) for word in sentence if tokenizer.decode(word) != \"<pad>\"] for sentence in sentences.cpu()]\n",
    "\n",
    "    model.zero_grad()\n",
    "    pred = model(sentences, attention_mask=masks).logits.detach().cpu()\n",
    "    pred_ind = pred.argmax(dim=-1)\n",
    "\n",
    "    reference_indices = token_reference.generate_reference(tokenizer.model_max_length, device=device).unsqueeze(0)\n",
    "\n",
    "    # compute attributions and approximation delta using layer integrated gradients\n",
    "    attributions, delta = lig.attribute(sentences, \n",
    "                                        reference_indices, \n",
    "                                        target=1, \n",
    "                                        additional_forward_args=(masks),\n",
    "                                        n_steps=500, \n",
    "                                        return_convergence_delta=True, \n",
    "                                        internal_batch_size=24\n",
    "    )\n",
    "\n",
    "    vis_data_records = []\n",
    "    for i in range(len(sentences)):\n",
    "        print(f\"pred: {classes[pred_ind[i]]} ({pred[i][1]:.2f}), delta: {abs(delta[i])}\")\n",
    "\n",
    "        attr = attributions[i]\n",
    "        attr = attr.sum(dim=-1)\n",
    "        attr = attr / torch.norm(attr)\n",
    "        attr = attr.cpu().detach().numpy()\n",
    "\n",
    "        # storing couple samples in an array for visualization purposes\n",
    "        vis_data_records.append(visualization.VisualizationDataRecord(\n",
    "                                attr,\n",
    "                                pred[i][1],\n",
    "                                classes[pred_ind[i]],\n",
    "                                classes[labels[i]],\n",
    "                                classes[1],\n",
    "                                attr.sum(),\n",
    "                                text[i],\n",
    "                                delta[i]))\n",
    "\n",
    "    return vis_data_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_data(\"test_generalization.csv\", padding=\"max_length\", max_length=512)\n",
    "\n",
    "neg_indices = test.labels == 0\n",
    "pos_indices = test.labels == 1\n",
    "\n",
    "neg_tweets = test.tweets[neg_indices]\n",
    "neg_amasks = test.amasks[neg_indices]\n",
    "neg_labels = test.labels[neg_indices]\n",
    "\n",
    "pos_tweets = test.tweets[pos_indices]\n",
    "pos_amasks = test.amasks[pos_indices]\n",
    "pos_labels = test.labels[pos_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "sentences = pos_tweets[i*16:min((i+1)*16, len(pos_tweets))]\n",
    "masks = pos_amasks[i*16:min((i+1)*16, len(pos_tweets))]\n",
    "labels = pos_labels[i*16:min((i+1)*16, len(pos_tweets))]\n",
    "\n",
    "pos_data_records = interpret_sentences(model, sentences, masks, labels)\n",
    "_ = visualization.visualize_text(pos_data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "sentences = neg_tweets[i*16:min((i+1)*16, len(neg_tweets))]\n",
    "masks = neg_amasks[i*16:min((i+1)*16, len(neg_tweets))]\n",
    "labels = neg_labels[i*16:min((i+1)*16, len(neg_tweets))]\n",
    "\n",
    "neg_data_records = interpret_sentences(model, sentences, masks, labels)\n",
    "visualization.visualize_text(neg_data_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
